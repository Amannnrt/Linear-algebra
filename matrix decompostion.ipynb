{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "179e7707-6aa5-43ea-baab-ba22b3082b67",
   "metadata": {},
   "source": [
    "# Matrix Decomposition\n",
    "\n",
    "Matrix decomposition is a fundamental concept in linear algebra that breaks down a matrix into simpler components. These components often reveal important properties of the original matrix, making it easier to solve systems of equations, analyze data, or perform numerical computations.\n",
    "\n",
    "In this notebook, we’ll explore key types of matrix decompositions:\n",
    "\n",
    "- **Eigen Decomposition and Diagonalizability**\n",
    "- **Singular Value Decomposition (SVD)**\n",
    "- **Cholesky Decomposition**\n",
    "\n",
    "## 1. Eigen Decomposition and Diagonalizability\n",
    "\n",
    "### Definition of Diagonalizability\n",
    "\n",
    "A square matrix $( A \\in \\mathbb{R}^{n \\times n} )$ is said to be diagonalizable if it is similar to a diagonal matrix. Mathematically, this means there exists an invertible matrix ($ P \\in \\mathbb{R}^{n \\times n} )$ such that:\n",
    "\n",
    "$$ D = P^{-1} A P $$\n",
    "\n",
    "where \\( D \\) is a diagonal matrix.\n",
    "\n",
    "This transformation expresses the same linear mapping but in another basis, which consists of the eigenvectors of \\( A \\).\n",
    "\n",
    "\n",
    "\n",
    "Let \\( A \\in \\mathbb{R}^{n \\times n} \\), and let \\( \\lambda_1, \\dots, \\lambda_n \\) be the eigenvalues with corresponding eigenvectors \\( p_1, \\dots, p_n \\). Define:\n",
    "\n",
    "$$ P = [p_1, \\dots, p_n] $$\n",
    "$$ D = \\text{diag}(\\lambda_1, \\dots, \\lambda_n) $$\n",
    "\n",
    "Then,\n",
    "\n",
    "$$ A P = P D $$\n",
    "\n",
    "which implies:\n",
    "\n",
    "$$ A p_i = \\lambda_i p_i, \\quad i = 1, \\dots, n $$\n",
    "\n",
    "### Conditions for Diagonalizability\n",
    "\n",
    "For \\( P \\) to be invertible (and hence \\( A \\) to be diagonalizable), the eigenvectors \\( p_1, \\dots, p_n \\) must form a basis of \\( \\mathbb{R}^n \\). This requires \\( A \\) to have \\( n \\) linearly independent eigenvectors.\n",
    "\n",
    "#### **Theorem: Symmetric Matrices are Always Diagonalizable**\n",
    "\n",
    "A symmetric matrix \\$( S \\in \\mathbb{R}^{n \\times n} \\)$ can always be diagonalized. Moreover, the spectral theorem guarantees that we can find an orthonormal basis of eigenvectors. In this case, \\( P \\) becomes an orthogonal matrix, and the diagonalization simplifies to:\n",
    "\n",
    "$$ D = P^T A P $$\n",
    "\n",
    "### Geometric Intuition for Eigen Decomposition\n",
    "\n",
    "Eigen decomposition of a matrix \\( A \\) can be interpreted geometrically:\n",
    "\n",
    "- $( P^{-1} )$ performs a basis change to the eigenbasis.\n",
    "- $( D )$ scales vectors along the eigenbasis by the eigenvalues \\( \\lambda_i \\).\n",
    "- $( P)$ transforms these scaled vectors back into the standard coordinate system.\n",
    "\n",
    "This process is useful for computing powers of \\( A \\):\n",
    "\n",
    "$$ A^k = (P D P^{-1})^k = P D^k P^{-1} $$\n",
    "\n",
    "Since \\( D \\) is diagonal, computing \\( D^k \\) involves simply raising each diagonal element to the power \\( k \\).\n",
    "\n",
    "#### Determinant via Eigen Decomposition\n",
    "\n",
    "If \\( A = P D P^{-1} \\), then:\n",
    "\n",
    "$$ \\det(A) = \\det(P D P^{-1}) = \\det(D) $$\n",
    "\n",
    "where:\n",
    "\n",
    "$$ \\det(A) = \\prod_{i=1}^{n} \\lambda_i $$\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Singular Value Decomposition (SVD)\n",
    "\n",
    "### Definition of SVD\n",
    "\n",
    "The Singular Value Decomposition (SVD) decomposes any matrix $(A \\in \\mathbb{R}^{m \\times n})$  as:\n",
    "\n",
    "$$ A = U (\\Sigma) V^T $$\n",
    "\n",
    "\n",
    "where:\n",
    "\n",
    "- $( U \\in \\mathbb{R}^{m \\times m} ) $is an orthogonal matrix $(( U^T U = I )).$\n",
    "- $( \\Sigma \\in \\mathbb{R}^{m \\times n} )$ is a diagonal matrix with non-negative singular values.\n",
    "- $( V \\in \\mathbb{R}^{n \\times n} )$ is an orthogonal matrix $(( V^T V = I )).$\n",
    "\n",
    "### Geometric Intuition for SVD\n",
    "\n",
    "SVD can be interpreted as a sequence of three transformations:\n",
    "\n",
    "1. **Basis Change in Domain:** $ V^T $transforms vectors into a new basis.\n",
    "2. **Scaling:** $ ( \\Sigma )$ scales the transformed coordinates.\n",
    "3. **Basis Change in Codomain:** $( U )$ maps the scaled coordinates into the output space.\n",
    "\n",
    "### Comparison with Eigen Decomposition\n",
    "\n",
    "| Property | Eigen Decomposition | Singular Value Decomposition |\n",
    "|----------|--------------------|-----------------------------|\n",
    "| Applicability | Only for square matrices | Works for any \\( m \\times n \\) matrix |\n",
    "| Basis Vectors | Eigenvectors may not be orthogonal | Left and right singular vectors are orthonormal |\n",
    "| Transformation | Operates within the same vector space | Links domain and codomain via \\( \\Sigma \\) |\n",
    "\n",
    "### Applications of SVD\n",
    "\n",
    "- **Dimensionality Reduction:** Truncate small singular values to reduce matrix rank.\n",
    "- **Image Compression:** Retain only the largest singular values.\n",
    "- **Least Squares Problems:** Solve overdetermined systems using the pseudoinverse.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Cholesky Decomposition\n",
    "\n",
    "### What is Cholesky Decomposition?\n",
    "\n",
    "Cholesky decomposition factors a symmetric positive-definite matrix \\( A \\) into the product of a lower triangular matrix \\( L \\) and its transpose:\n",
    "\n",
    "$$ A = L L^T $$\n",
    "\n",
    "where \\( L \\) is a lower triangular matrix with positive diagonal entries.\n",
    "\n",
    "### When is it Used?\n",
    "\n",
    "- **Solving linear systems efficiently**\n",
    "- **Simulating multivariate normal distributions**\n",
    "- **Optimizing quadratic forms in optimization problems**\n",
    "\n",
    "### Mathematical Formulation\n",
    "\n",
    "For a symmetric positive-definite matrix \\( A \\), Cholesky decomposition is computed as follows:\n",
    "\n",
    "#### **Compute diagonal elements of \\( L \\):**\n",
    "\n",
    "$$ L_{ii} = \\sqrt{A_{ii} - \\sum_{k=1}^{i-1} L_{ik}^2} $$\n",
    "\n",
    "#### **Compute off-diagonal elements of \\( L \\):**\n",
    "\n",
    "$$ L_{ij} = \\frac{1}{L_{jj}} \\left(A_{ij} - \\sum_{k=1}^{j-1} L_{ik} L_{jk} \\right), \\quad j < i $$\n",
    "\n",
    "### Example\n",
    "\n",
    "Let’s decompose the symmetric positive-definite matrix:\n",
    "\n",
    "$$ A = \\begin{bmatrix} 4 & 12 & -16 \\\\ 12 & 37 & -43 \\\\ -16 & -43 & 98 \\end{bmatrix} $$\n",
    "\n",
    "Using Cholesky decomposition, we find:\n",
    "\n",
    "$$ L = \\begin{bmatrix} 2 & 0 & 0 \\\\ 6 & 1 & 0 \\\\ -8 & 5 & 3 \\end{bmatrix} $$\n",
    "\n",
    "Cholesky decomposition is computationally efficient and widely used in numerical methods for solving linear systems.\n",
    "\n",
    "### **Summary of Decompositions**\n",
    "| Decomposition  | Matrix Type | Form | Used For |\n",
    "|---------------|------------|------|---------|\n",
    "| **Eigen Decomposition** | Square matrices with \\( n \\) independent eigenvectors | \\( A = P D P^{-1} \\) | Diagonalization, power computations, PCA |\n",
    "| **SVD** | Any matrix \\( A \\in \\mathbb{R}^{m \\times n} \\) | \\( A = U \\Sigma V^T \\) | Dimensionality reduction, compression, least squares |\n",
    "| **Cholesky Decomposition** | Symmetric positive-definite matrices | \\( A = L L^T \\) | Solving linear systems, optimization, statistics |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "591b8aa8-0607-4459-9c69-38e0d1b38926",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cholesky Decomposition:\n",
      " [[ 2.  0.  0.]\n",
      " [ 6.  1.  0.]\n",
      " [-8.  5.  3.]]\n",
      "\n",
      "Eigenvalues:\n",
      " [3. 1.]\n",
      "Eigenvectors:\n",
      " [[ 0.70710678 -0.70710678]\n",
      " [ 0.70710678  0.70710678]]\n",
      "\n",
      "SVD U:\n",
      " [[ 0.70710678 -0.70710678]\n",
      " [ 0.70710678  0.70710678]]\n",
      "Singular Values:\n",
      " [5. 3.]\n",
      "SVD Vt:\n",
      " [[ 7.07106781e-01  7.07106781e-01  3.88578059e-16]\n",
      " [-2.35702260e-01  2.35702260e-01 -9.42809042e-01]\n",
      " [-6.66666667e-01  6.66666667e-01  3.33333333e-01]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def cholesky_decomposition(A):\n",
    "    \"\"\"Computes the Cholesky decomposition of a positive-definite matrix A.\"\"\"\n",
    "    L = np.linalg.cholesky(A)\n",
    "    return L\n",
    "\n",
    "def eigen_decomposition(A):\n",
    "    \"\"\"Computes the eigen decomposition of a square matrix A.\"\"\"\n",
    "    eigenvalues, eigenvectors = np.linalg.eig(A)\n",
    "    return eigenvalues, eigenvectors\n",
    "\n",
    "def singular_value_decomposition(A):\n",
    "    \"\"\"Computes the Singular Value Decomposition (SVD) of matrix A.\"\"\"\n",
    "    U, S, Vt = np.linalg.svd(A)\n",
    "    return U, S, Vt\n",
    "\n",
    "# Example matrices\n",
    "A_cholesky = np.array([[4, 12, -16], [12, 37, -43], [-16, -43, 98]])\n",
    "A_eigen = np.array([[2, 1], [1, 2]])\n",
    "A_svd = np.array([[3, 2, 2], [2, 3, -2]])\n",
    "\n",
    "# Compute decompositions\n",
    "L = cholesky_decomposition(A_cholesky)\n",
    "eigenvalues, eigenvectors = eigen_decomposition(A_eigen)\n",
    "U, S, Vt = singular_value_decomposition(A_svd)\n",
    "\n",
    "# Display results\n",
    "print(\"Cholesky Decomposition:\\n\", L)\n",
    "print(\"\\nEigenvalues:\\n\", eigenvalues)\n",
    "print(\"Eigenvectors:\\n\", eigenvectors)\n",
    "print(\"\\nSVD U:\\n\", U)\n",
    "print(\"Singular Values:\\n\", S)\n",
    "print(\"SVD Vt:\\n\", Vt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f22c6f-24a5-4179-a145-cca3e25e1b64",
   "metadata": {},
   "source": [
    "\n",
    "![SVD Decomposition](image1.png)\n",
    "\n",
    "\n",
    "The given image illustrates the Singular Value Decomposition (SVD) of a **movie rating matrix**. It represents ratings given by three users (Ali, Beatrix, Chandra) to four movies (Star Wars, Blade Runner, Amelie, Delicatessen).\n",
    "\n",
    "*image taken from the book mathematics for machine learning*\n",
    "### **Step 1: Original Matrix Representation**\n",
    "The original matrix \\( A \\) contains movie ratings:\n",
    "\n",
    "$$[\n",
    "A =\n",
    "\\begin{bmatrix}\n",
    "5 & 4 & 1 \\\\\n",
    "5 & 5 & 0 \\\\\n",
    "0 & 0 & 5 \\\\\n",
    "1 & 0 & 4\n",
    "\\end{bmatrix}\n",
    "]$$\n",
    "\n",
    "Each row corresponds to a movie, and each column corresponds to a user.\n",
    "\n",
    "### **Step 2: SVD Factorization**\n",
    "SVD decomposes the matrix \\( A \\) into three matrices:\n",
    "\n",
    "$[\n",
    "A = U \\Sigma V^T\n",
    "]$\n",
    "\n",
    "where:\n",
    "\n",
    "- $( U )$ (left singular vectors): Represents movie preferences in a transformed space.\n",
    "- $( \\Sigma )$ (singular values): Contains the importance (or strength) of each singular component.\n",
    "- $( V^T )$ (right singular vectors): Represents user preferences in a transformed space.\n",
    "\n",
    "#### **Matrix \\( U \\) (Left Singular Vectors)**\n",
    "$$[\n",
    "U =\n",
    "\\begin{bmatrix}\n",
    "\\color{red} -0.6710 & 0.0236 & 0.4647 & -0.5774 \\\\\n",
    "\\color{red} -0.7197 & 0.2054 & -0.4759 & 0.4619 \\\\\n",
    "-0.0939 & \\color{blue} -0.7705 & -0.5268 & -0.3464 \\\\\n",
    "-0.1515 & \\color{blue} -0.6030 & 0.5293 & -0.5774\n",
    "\\end{bmatrix}\n",
    "]$$\n",
    "\n",
    "- The **first two columns** (highlighted in red and blue) represent the most significant singular vectors, capturing most of the variation in the dataset.\n",
    "- These vectors **represent a transformed basis for the movies** in terms of hidden factors (e.g., \"sci-fi vs. drama\" or \"classic vs. modern\").\n",
    "\n",
    "#### **Matrix \\( \\Sigma \\) (Singular Values)**\n",
    "$$[\n",
    "\\Sigma =\n",
    "\\begin{bmatrix}\n",
    "\\color{red} 9.6438 & 0 & 0 & 0 \\\\\n",
    "0 & \\color{blue} 6.3639 & 0 & 0 \\\\\n",
    "0 & 0 & 0.7056 & 0 \\\\\n",
    "0 & 0 & 0 & 0\n",
    "\\end{bmatrix}\n",
    "]$$\n",
    "\n",
    "- The **largest singular value (9.6438, in red)** represents the most significant pattern in the ratings.\n",
    "- The **second singular value (6.3639, in blue)** captures the next most important pattern.\n",
    "- The last two singular values are much smaller, meaning they contribute less to the structure of the data.\n",
    "- This suggests that the data can be **approximated well using just the first two singular vectors**, making SVD useful for dimensionality reduction.\n",
    "\n",
    "#### **Matrix \\( V^T \\) (Right Singular Vectors)**\n",
    "$$[\n",
    "V^T =\n",
    "\\begin{bmatrix}\n",
    "\\color{green} -0.7367 & -0.6515 & -0.1811 \\\\\n",
    "0.0852 & 0.1762 & \\color{yellow} -0.9807 \\\\\n",
    "0.6708 & -0.7379 & -0.0743\n",
    "\\end{bmatrix}\n",
    "]$$\n",
    "\n",
    "- The rows represent **users** in a transformed space.\n",
    "- The first row (highlighted in green) represents the dominant user preference pattern.\n",
    "- The second row (highlighted in yellow) captures a secondary pattern.\n",
    "- This means that **users can also be grouped based on their rating behaviors**.\n",
    "\n",
    "### **Interpretation: Finding Hidden Patterns**\n",
    "- The SVD decomposition suggests that the **movie ratings can be explained using just two main underlying factors** instead of three.\n",
    "- The singular values indicate that most of the data variation is captured in the first two components, meaning we could **compress this data** while still retaining most of the information.\n",
    "- This technique is widely used in **recommendation systems**, where user preferences can be predicted based on hidden patterns in past ratings.\n",
    "\n",
    "### **Key Takeaways**\n",
    "- **\\( U \\)** represents transformed movies.\n",
    "- $( \\Sigma ) $  shows the strength of each pattern.\n",
    "- **\\( V^T \\)** represents transformed users.\n",
    "- The **largest singular values** highlight the most important structures in the data.\n",
    "- **Dimensionality reduction**: Since smaller singular values contribute little, we can approximate the original matrix using only the first two singular values.\n",
    "\n",
    "This SVD decomposition helps in understanding user preferences, reducing noise, and making better recommendations.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e36f2d-41e3-4b97-92ec-8e90c1f8cd62",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
